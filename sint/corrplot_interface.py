# -*- coding: utf-8 -*-

"""
simulation interface

based on LCLSMachineInterface - Machine interface file for the LCLS to ocelot optimizer

"""

import numpy as np
#import epics
import time
import sys
import math
#from corrplot_interface import CorrplotInterface # pasted below
#import numpy as np
from numpy.random import rand
from scipy.interpolate import griddata # for interpolating data on a grid for the maps
from corrplot import *
from fitcorrplot import *

# an interface that interpolates a function over a grid over a certain domain and extrapolates based on a fit outside of that domain
#_____________________________________________
class CorrplotInterface:
#class CorrplotInterface(object):
        # takes a corrplot .mat file pathToCorrPlotFile
    """ Start acquisition interface class """

    #_____________________________________________
    def __init__(self, pathToCorrPlotFile = '/u1/lcls/matlab/data/2016/2016-10/2016-10-20/CorrelationPlot-QUAD_IN20_511_BCTRL-2016-10-20-053315.mat', fitOnlyQ=False, drunkenQ=False):
        # interface name
        self.name = 'CorrplotInterface'
        
        # default statistic to tune on
        self.stat_name = 'Mean'
        
        """ Initialize parameters for the scanner class. """
        self.secs_to_ave = 2         #time to integrate gas detector

        self.points = 1
        self.nsamples = self.points
        self.simmode = 1 # 0: multinormal distribution
                         # 1: correlation plot fit
        self.quickNoiseQ = False # if True: only one sample is drawn with uncert = standard error of the mean
        self.pathToCorrPlotFile = pathToCorrPlotFile

        # set up interpolation

        # import data
        #[nby4arrayoffloats, legend] = corrplot(pathToCorrPlotFile) # read in corrplot data
        [nby4arrayoffloats, legend, self.ebeam_energy, self.photon_energy] = corrplot(pathToCorrPlotFile) # read in corrplot data
        nby4arrayoffloats = np.transpose(nby4arrayoffloats)
        self.ebeam_energy = self.ebeam_energy / 1000.

        self.pvs = legend
        self.pvdict = dict() # for simple lookup
        for i in range(len(self.pvs)):
            self.pvdict[self.pvs[i]] = i # objective fcn is last here

        print "\nCorrplotInterface: PVs imported from correlation plot data:"
        for pv in self.pvs: print(pv)
        print
        
        #self.ebeam_energy = ebeam_energy # FIX ME
        print 'self.ebeam_energy = ',self.ebeam_energy
        print 'self.photon_energy = ',self.photon_energy

        # takes a numpy array of grid points with cols x, y, mean(fel), stdev(fel)
        self.samples = nby4arrayoffloats[:,:2]
        self.means = nby4arrayoffloats[:,2]
        self.stdevs = nby4arrayoffloats[:,3]
        self.mins = [min(self.samples[:,a]) for a in range(len(self.samples[0]))];
        self.maxes = [max(self.samples[:,a]) for a in range(len(self.samples[0]))];

        #print("mins = " + str(self.mins))
        #print("maxes = " + str(self.maxes))

        #print('CorrplotInterface: self.samples = ' + str(self.samples))
        #print('CorrplotInterface: self.means = ' + str(self.means))
        #print('CorrplotInterface: self.stdevs = ' + str(self.stdevs))
        #if len(nby4arrayoffloats[0]) == 4:
        #    self.stdevs = [a[3] for a in nby4arrayoffloats]
        #else:
        #    self.stdevs = [0 for a in nby4arrayoffloats]
        # seems like in fint, he wants to store the last random number generated by the last fcn call so let's store it
        #self.x = [0, 0]
        self.x = np.array([18.,-16.5],ndmin=2)
        self.y = np.array(-1, ndmin=2)
        self.mean = 0
        self.stdev = 0
        self.noise = self.stdev
        self.drunkenQ = drunkenQ
        self.noiseScaleFactor = 1.;
        self.fitOnlyQ = fitOnlyQ;

        # set up extrapolation

        # fit binormal variate distribution and return params
        # note params order: ['amp','xm','sx','ym','sy','rho']
        fitparams2 = fitcorrplot(pathToCorrPlotFile)
        self.mean_amp = fitparams2[0]
        self.mean_xm = fitparams2[1]
        self.mean_sx = fitparams2[2]
        self.mean_ym = fitparams2[3]
        self.mean_sy = fitparams2[4]
        self.mean_rho = fitparams2[5]

        # repeat the fit for the distribution of fluctuations instead
        fitparams = fitcorrplot(pathToCorrPlotFile, fitNoiseQ=True)
        self.stdev_amp = fitparams[0]
        self.stdev_xm = fitparams[1]
        self.stdev_sx = fitparams[2]
        self.stdev_ym = fitparams[3]
        self.stdev_sy = fitparams[4]
        self.stdev_rho = fitparams[5]
        self.stdev_bg = fitparams[6]

        #if(self.simmode == 1 or True):
        #self.sim = CorrplotInterface(self.pathToCorrPlotFile)
        self.getter = self     #getter class for channel access
        self.detector = self.pvs[-1]

        # reference for goal
        self.pvs_optimum_value = np.array([[self.mean_xm, self.mean_ym], self.mean_amp])
        self.detector_optimum_value = self.pvs_optimum_value[-1]
        
        # start point
        self.randomkick_zscore = 1.
        print 'CorrplotInterface: starting ',self.randomkick_zscore,' sigma from the peak'
        randomkick = self.randomkick_zscore * np.random.randn(2) * np.array([self.mean_sx,self.mean_sy])
        self.x = np.round(np.array(self.pvs_optimum_value[0]+randomkick,ndmin=2),2)


    ############################################################
    # main public member functions


    #_____________________________________________
    # The UI requests how much time we need to trim magnets
    def dataDelay(self, objective_func_pv, numPulse):
        self.points = numPulse
        self.nsamples = self.points;
        return 0.000001

    #_____________________________________________
    # Something LCLS specific that should probably be contained in LCLSMachineInterface.__init__
    def setListener(self, anyNumber):
        pass

    #_____________________________________________
    # Something LCLS specific that should probably be contained in LCLSMachineInterface.__init__
    def get_charge_current(self):
        charge = None
        current = None
        return charge, current

    #_____________________________________________
    # Get fcn takes 1 pv string or a list of pvs
    # In a real machine setting, this fcn should check the machine status before returning values
    def get_value(self, variable_names):

        # if a list of names, return a list of values
        #if hasattr(variable_names, '__iter__'):
        #    print 'getting a value', self.get1(var)
        #    return np.array([self.get1(var) for var in variable_names])
        #return self.get1(var)
        # if one variable, return one value
        #else:
        return self.get1(variable_names)

    #_____________________________________________
    # Get fcn takes 1 pv string or a list of pvs
    # In a real machine setting, this fcn should check the machine status and wait for devices to settle before returning values
    def set_value(self, variable_names, values):

        # if a list of names, return a list of values
        #if hasattr(variable_names, '__iter__') and hasattr(values, '__iter__'):
        #    if len(variable_names) == len(values):
        #        for i in range(len(variable_names)):
        #            self.set1(variable_names[i], values[i])
        #    else:
        #        print "SimulationInterface.get - ERROR: Inputs must have same number of entries."
        #
        #elif hasattr(variable_names, '__iter__') or hasattr(values, '__iter__'):
        #    print "SimulationInterface.get - ERROR: Inputs must both be listable."

        # if one variable, set one value
        #else:
        #    print 'setting a value', values
        self.set1(variable_names, values)

    #def initErrorCheck(self): # not needed (call errorCheck or something or do this in the setup from the constructor
            #"""
            #Initialize PVs and setting used in the errorCheck method.
            #"""
            #return

    def get(self, pv):
        #return self.get_value(pv)
        print 'getting', pv, self.get1(pv)
        return self.get1(pv)

    def get_energy(self):
        return self.ebeam_energy

    def put(self, pv, val):
        print 'setting', pv, self.set1(pv, val)

    def errorCheck(self):
    #def get_status(self): # how is errorCheck used? maybe you want a get_status fcn instead: OK, WAIT, BREAK
        """
        Method that check the state of BCS, MPS, Gaurdian, UND-TMIT and pauses GP if there is a problem.
        """
        return

    #def get_sase(self):
    #def get_objective(self, nsamples = 100, returnErrorQ = False): # where is it best to set this up?
    def get_sase(self, data, points = None, returnErrorQ = False):
        """
        Returns an average for the objective function from the selected detector PV.

        Args:
                nsamples (int): integer number of samples to average over
                returnErrorQ (bool): option to return stdev too

        Returns:
                Float of detector measurment mean (and optionally standard deviation)

        Ideas:  - option to do median & median deviation
                - perhaps we'd prefer to specify a precision tolerance? i.e. sample until abs(mean(data[-n:]) - mean(data[-(n-1):])) / std(data[-n:]) < tolerance  or  abs(data[-n] / n) / std(data) < tolerance

        """

        print 'points = ',points,'\tself.points = ',self.points,'\tdata.size = ',data.size
        # update number of points to sample
        if(points != self.points):
            self.points = points
            self.nsamples = self.points
            data = self.f(self.x)
        print 'points = ',points,'\tself.points = ',self.points,'\tdata.size = ',data.size

        # NOTE: the following isn't needed cause noise is scaled by 1/sqrt(self.points) in self.get_value which is called by sint.opt_objects.SLACTarget.get_value and then passed to self.get_sase

        ## acquire data
        #data = np.array([self.get_value(self.detector) for i in range(points)])

        ## perform stats
        #try:
            #datamean = np.mean(data)
            #datadev  = np.std(data)
            #print([datamean,datadev,datadev/datamean])
        #except:
            #datamean = data
            #datadev = data
            #print([datamean,datadev,datadev/datamean])

        # record stats
        #self.record_data(datamean,datadev) # do we want the ctrl interface to record?

        ## return stats
        #if returnErrorQ:
            #return np.array([datamean, datadev])
        #else:
            #return datamean, datadev


        #print "objective mean and stdev for " + str(self.points) + " acquired points is " + str([np.mean(data), self.stdev])

        #return np.mean(data), self.stdev
        if self.quickNoiseQ is True:

            # NOTE: self.stdev is standard deviation for 1 sample
            #       self.stderr_of_mean is the standard error of the mean

            data = self.f(self.x)

            print "objective mean, stderr_of_mean, and snr for " + str(self.points) + " acquired points is " + str([np.mean(data), self.stderr_of_mean, np.mean(data)/self.stderr_of_mean])

            #print "self.stdev[0,0] = ",self.stdev[0,0],"\tself.bgNoise = ",self.bgNoise,"\nself.stdev[0,0]/self.bgNoise*np.sqrt(self.points) = ",self.stdev[0,0]/self.bgNoise*np.sqrt(self.points)
            #print "np.std(data) = ", np.std(data),"/ndata = ",data

            #return np.mean(data), self.stdev

            # sample standard deviation and standard error of the sample stdev
            #print 'gamma(points/2.) = ',gamma(points/2.),'\ngamma((points-1.)/2.) = ',gamma((points-1.)/2.)
            c4n = np.sqrt(2./(points-1.))*gamma(points/2.)/gamma((points-1.)/2.)
            stdev_stderr = self.stdev[0,0] * np.sqrt(c4n**-2 - 1.)
            sample_stdev = self.stdev[0,0]+np.random.randn()*stdev_stderr
            stderr_of_mean = max([sample_stdev / np.sqrt(points), 1.e-10])
            #print 'points = ',points,'\nc4n = ',c4n,'\nstdev_stderr = ',stdev_stderr,'sample_stdev = ',sample_stdev
            #return sample_stdev / self.bgNoise - 1., sample_stdev # better suppression


            if self.stat_name == 'Mean' or self.stat_name == 'Median':
                return np.mean(data),stderr_of_mean
            elif self.stat_name == '80th percentile':
                # percentile cut
                pvalue = 0.8 # pvalue = percentile/100.
                zscore = np.sqrt(2) * erfinv(2*pvalue-1)
                return zscore * sample_stdev, stderr_of_mean
            elif self.stat_name == '20th percentile':
                # percentile cut
                pvalue = 0.2 # pvalue = percentile/100.
                zscore = np.sqrt(2) * erfinv(2*pvalue-1)
                return zscore * sample_stdev, stderr_of_mean
            elif self.stat_name == 'Standard deviation':
                return self.sample_stdev, stdev_stderr
            else:
                print "MultinormalInterface - WARNING: not sampling so ignoring gui set statistic for now. Returning mean"
                return np.mean(data),stderr_of_mean

        else:

            data = np.array([self.f(self.x) for i in range(int(points))])
            #data = self.f(self.x)

            #print "points = ", points, "\nself.points = ",self.points,"\ndata = ", data
            #print "data = ", data
            #print "data.shape = ",data.shape,"\tdata.size = ",data.size,"\tnp.std(data) = ",np.std(data)

            #return np.mean(data), np.std(data)
            #return np.std(data) / self.bgNoise - 1, np.std(data)
            #return np.max(data), np.std(data)
            #return np.mean(data[data>np.mean(data)]), np.std(data)
            #return np.percentile(data, 90), np.std(data)

            if self.stat_name == 'Median':
                statistic = np.median(data)
            elif self.stat_name == 'Standard deviation':
                statistic = np.std(data)
            elif self.stat_name == 'Median deviation':
                median = np.median(data)
                statistic = np.median(np.abs(data-median))
            elif self.stat_name == 'Max':
                statistic = np.max(data)
            elif self.stat_name == 'Min':
                statistic = np.min(data)
            elif self.stat_name == '80th percentile':
                statistic = np.percentile(data,80)
            elif self.stat_name == 'average of points > mean':
                dat_last = data
                percentile = np.percentile(data,50)
                statistic = np.mean(dat_last[dat_last>percentile])
            elif self.stat_name == '20th percentile':
                statistic = np.percentile(data,20)
            else:
                self.stat_name = 'Mean'
                statistic = np.mean(data)
            # check if this is even used:
            sigma   = np.std(data)

            print self.stat_name, ' of ', data.size, ' points is ', statistic, ' and standard deviation is ', sigma

            return statistic, sigma

    def get_limits(self, device,percent=0.25):
        """
        Function to get device limits.
        Executes on every iteration of the optimizer function evaluation.
        Currently does not work with the normalization scheme.
        Defaults to + 25% of the devices current values.

        Args:
            device (str): PV name of the device to get a limit for
            percent (float): Generates a limit based on the percent away from the devices current value
        """
        #val = self.start_values[device]
        val = self.get(device)
        tol = abs(val*percent)
        lim_lo = val-tol
        lim_hi = val+tol
        limits = [lim_lo,lim_hi]
        #print device, 'LIMITS ->',limits
        return limits

        ##Dosnt work with normalizaiton, big limits
        #return [-1,1] # looks like this function doesn't do anything

    # looks redundant
    #def get_start_values(self,devices,percent=0.25):
        #"""
        #Function to initialize the starting values for get_limits methomethodd.

        #Called from tuning file or GUI

        #Args:
                #devices ([str]): PV list of devices
                #percent (float): Percent around the mean to generate limits

        #"""
        #self.start_values={}
        #self.norm_minmax={}
        #for d in devices:
                #val = self.getter.caget(str(d))
                #self.start_values[str(d)] = val
                #tol = abs(val*percent)
                #lim_lo = val-tol
                #lim_hi = val+tol
                #limits = [lim_lo,lim_hi]
                #self.norm_minmax[str(d)] = [lim_lo,lim_hi]

    # END LCLSMachineInterface specific functions

    # simple access fcn
    def get1(self, pvname):

        index = self.pvdict[pvname]
        if index == len(self.pvs)-1:
            self.getState()
            if hasattr(self.y, '__iter__'):
                return self.y[0]
            else:
                return self.y
        else:
            return self.x[-1,index]

    # simple access fcn
    def set1(self, pvname, value):
        index = self.pvdict[pvname]
        if index == len(self.pvs)-1:
            self.y = value
        else:
            self.x[-1,index] = value

    # interpolating function
    def fin(self, x_new): # let this method update means and stdevs
        #print("interpolation")
        self.x = x_new
        grid_x, grid_y = np.mgrid[self.x[0,0]:1:1j, self.x[0,1]:1:1j]
        self.mean = griddata(self.samples, self.means, (grid_x, grid_y), method='cubic')[0][0]
        self.stdev = griddata(self.samples, self.stdevs, (grid_x, grid_y), method='cubic')[0][0]

        # scale noise to avoid multi samples
        self.stdev = abs(self.noiseScaleFactor * self.stdev)/np.sqrt(self.points) + 1.e-10
        # could sample the stdev as well but prob doesn't matter

        # perturb mean by noise
        self.y = np.array(np.random.normal(self.mean, self.stdev, self.mean.shape), ndmin=2)  # zero noise where the mean is

        return np.array(self.y, ndmin=2)


    # extrapolating function
    def fex(self, x_new): # let this method update means and stdevs
        #print("extrapolation")
        self.x = x_new
        x = np.array([te[0] for te in x_new])
        y = np.array([te[1] for te in x_new])

        amp = self.mean_amp
        xm = self.mean_xm
        sx = self.mean_sx
        ym = self.mean_ym
        sy = self.mean_sy
        rho = self.mean_rho
        self.mean = amp * np.exp(-0.5*((x-xm)*(x-xm)/sx/sx+(y-ym)*(y-ym)/sy/sy-2.*rho*(x-xm)*(y-ym)/sx/sy)/(1.-rho*rho))

        #bg = self.stdev_bg # need to modify fitcorrplot to get this
        amp = self.stdev_amp
        xm = self.stdev_xm
        sx = self.stdev_sx
        ym = self.stdev_ym
        sy = self.stdev_sy
        rho = self.stdev_rho
        bg = self.stdev_bg
        self.stdev = bg + amp * np.exp(-0.5*((x-xm)*(x-xm)/sx/sx+(y-ym)*(y-ym)/sy/sy-2.*rho*(x-xm)*(y-ym)/sx/sy)/(1.-rho*rho))

        # scale noise to avoid multi samples
        self.stdev = abs(self.noiseScaleFactor * self.stdev)/np.sqrt(self.points) + 1.e-10
        # could sample the stdev as well but prob doesn't matter

        # perturb mean by noise
        self.y = np.array(np.random.normal(self.mean, self.stdev, self.mean.shape), ndmin=2)  # zero noise where the mean is

        return np.array(self.y, ndmin=2)

    # wrapper function to decide based on domain
    #def finex(self, x_new):
    def f(self, x_new):
        
        self.x = x_new
        
        # copy more stuff from multinormal interface??
        
        if self.fitOnlyQ:
            return self.fex(x_new)
        else:
            #decide whether inside or outside the domain
            withinQ = 0*x_new[0];
            #print(withinQ)
            for i in range(len(withinQ)):
                if (self.mins[i] <= x_new[0][i]) & (x_new[0][i] <= self.maxes[i]):
                    withinQ[i] = 1;
            withinQ = np.prod(withinQ); # if all coords are within the interp range prod is 1 else 0
            #print(withinQ)
            #print(np.prod(withinQ))
            #exit()
            #return self.fex(x_new)
            if(withinQ):
                return self.fin(x_new)
            else:
                return self.fex(x_new)

    # wrapper function to decide extrapolation vs interpolation plus extrapolation
    #def f(self, x_new):
        #if self.fitOnlyQ:
            #return self.fex(x_new)
        #else:
            #return self.finex(x_new)

    def getState(self): # see note at top regarding
        #return self.x, self.y, self.mean, self.stdev
        #return self.x, self.f(self.x)
        #return np.array(self.x, ndmin = 2), self.fex(self.x)
        return np.array(self.x, ndmin = 2), self.f(self.x)

    def setX(self, x_new):
        self.x = x_new
        #print("BasicInterfaces.CorrplotInterface.setX: self.x = " + str(self.x))
        if(self.drunkenQ and np.random.uniform() < 0.05):
            self.x = x_new + np.transpose([np.random.normal(0,0.25*self.mean_sx,len(x_new)),np.random.normal(0,0.25*self.mean_sy,len(x_new))])
            print("CorrplotInterface.setX: self.x = " + str(self.x) + " (stutter)")


    #=======================================================#
    # ------------------- Data Saving --------------------- #
    #=======================================================#

    def recordData(self, objective_func_pv, objective_func, devices):
        """
        Get data for devices everytime the SASE is measured to save syncronous data.

        Args:
                gdet (str): String of the detector PV, usually gas detector
                simga (float): Float of the measurement standard deviation

        """
        self.data = {} #dict of all devices deing scanned
        self.data[objective_func_pv] = [] #detector data array
        self.data['DetectorStd'] = [] #detector std array
        self.data['timestamps']  = [] #timestamp array
        self.data['charge']=[]
        self.data['current'] =[]
        self.data['stat_name'] =[]
        for dev in devices:
            self.data[dev.eid] = []
        #print('obj times', objective_func.times)
        for dev in devices:
            vals = len(dev.values)
            self.data[dev.eid].append(dev.values)
        if vals<len(objective_func.values):
            objective_func.values = objective_func.values[1:]
            objective_func.times = objective_func.times[1:]
            objective_func.std_dev = objective_func.std_dev[1:]
            objective_func.charge = objective_func.charge[1:]
            objective_func.current = objective_func.current[1:]
        self.data[objective_func_pv].append(objective_func.values)
        self.data['DetectorStd'].append(objective_func.std_dev)
        self.data['timestamps'].append(objective_func.times)
        self.data['charge'].append(objective_func.charge)
        self.data['current'].append(objective_func.current)
        self.data['stat_name'].append(self.stat_name)
        return self.data

    def saveData(self, objective_func_pv, objective_func, devices, name_opt, norm_amp_coeff):
        """
        Save scan data to the physics matlab data directory.

        Uses module matlog to save data dict in machine interface file.
        """
        data_new = self.recordData(objective_func_pv, objective_func, devices)
        #get the first and last points for GDET gain
        self.detValStart = data_new[objective_func_pv][0]
        self.detValStop  = data_new[objective_func_pv][-1]

        #replace with matlab friendly strings
        for key in data_new:
            key2 = key.replace(":","_")
            data_new[key2] = data_new.pop(key)

        #extra into to add into the save file
        #data_new["BEND_DMP1_400_BDES"]   = self.get("BEND:DMP1:400:BDES")
        data_new["BEND_DMP1_400_BDES"]   = self.get_energy()
        data_new["Energy"]   = self.get_energy()
        data_new["ScanAlgorithm"]        = str(name_opt)      #string of the algorithm name
        data_new["ObjFuncPv"]            = str(objective_func_pv) #string identifing obj func pv
        data_new["NormAmpCoeff"]         = norm_amp_coeff

        #save data
        import simlog
        self.last_filename=simlog.save("OcelotScan",data_new,path='default')#self.save_path)
